{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptSnkV5J4EEl",
        "outputId": "627a690a-9dcd-4a9e-cfe3-36fa1781ce62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and cleaning data...\n",
            "Loaded 469 valid data points\n",
            "Calculating performance metrics...\n",
            "Creating summary table...\n",
            "Creating comparison table...\n",
            "\n",
            "================================================================================\n",
            "TTC ANALYSIS SUMMARY - TOP 10 PERFORMERS\n",
            "================================================================================\n",
            " Rank     Combination   MAE  RMSE  Relative_Error_%  Stability_StdDev  Success_Rate_%  Outlier_Rate_%  Avg_Keypoints  Avg_Matches\n",
            "    1      FAST+BRISK 1.609 1.941            14.922             0.870           100.0           0.000       5282.333     2241.000\n",
            "    2      FAST+FREAK 1.609 1.941            14.922             0.870           100.0           0.000       5282.333     2241.000\n",
            "    3      FAST+BRIEF 1.609 1.941            14.922             0.870           100.0           0.000       5282.333     2241.000\n",
            "    4        FAST+ORB 1.607 1.978            14.723             1.095           100.0           0.000       5282.333     2853.056\n",
            "    5      SIFT+FREAK 0.870 1.130             7.353             2.696           100.0           0.000       1983.056      774.667\n",
            "    6      SIFT+BRIEF 0.870 1.130             7.353             2.696           100.0           0.000       1983.056      774.667\n",
            "    7      SIFT+BRISK 0.870 1.130             7.353             2.696           100.0           0.000       1983.056      774.667\n",
            "    8     AKAZE+AKAZE 1.601 1.953            14.328             2.153           100.0           0.000       1548.000     1120.556\n",
            "    9 SHITOMASI+BRISK 2.071 2.464            19.229             1.411           100.0           6.667        838.600      365.000\n",
            "   10 SHITOMASI+BRIEF 2.071 2.464            19.229             1.411           100.0           6.667        838.600      365.000\n",
            "\n",
            "==================================================\n",
            "WORST 5 PERFORMERS\n",
            "==================================================\n",
            " Rank Combination    MAE   RMSE  Relative_Error_%  Stability_StdDev  Success_Rate_%  Outlier_Rate_%  Avg_Keypoints  Avg_Matches\n",
            "   24 BRISK+BRIEF  4.145  5.178            36.292             4.099           100.0          33.333       2988.389     1580.167\n",
            "   25   ORB+BRISK  9.570 16.233            79.459            14.810           100.0          40.000        500.000      311.600\n",
            "   26   ORB+BRIEF  9.570 16.233            79.459            14.810           100.0          40.000        500.000      311.600\n",
            "   27   ORB+FREAK  9.570 16.233            79.459            14.810           100.0          40.000        500.000      311.600\n",
            "   28     ORB+ORB 12.843 17.966           116.737            13.011           100.0          66.667        500.000      326.111\n",
            "\n",
            "Best performer: FAST+BRISK (MAE: 1.609s)\n",
            "Worst performer: ORB+ORB (MAE: 12.843s)\n",
            "Most stable: FAST+BRIEF (StdDev: 0.870s)\n",
            "Highest success rate: AKAZE+AKAZE (100.0%)\n",
            "Creating visualizations...\n",
            "Visualizations saved to ttc_analysis_plots/\n",
            "\n",
            "Analysis complete! Files saved:\n",
            "- ttc_summary_table.csv (ranked performance summary)\n",
            "- ttc_comparison_table.csv (frame-by-frame comparison)\n",
            "- ttc_detailed_metrics.csv (detailed metrics)\n",
            "- ttc_analysis_plots/ (visualization plots)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "def load_and_clean_data(csv_path):\n",
        "    \"\"\"Load CSV data and clean invalid entries\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Convert 'invalid' strings to NaN\n",
        "    for col in ['TTC_Lidar', 'TTC_Camera', 'TTC_Difference']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Remove rows where both TTC values are invalid\n",
        "    df = df.dropna(subset=['TTC_Lidar', 'TTC_Camera'], how='all')\n",
        "\n",
        "    # Filter out extreme outliers (likely computation errors)\n",
        "    df = df[df['TTC_Lidar'].between(0, 100)]  # Reasonable TTC range\n",
        "    df = df[df['TTC_Camera'].between(0, 100)]\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_performance_metrics(df):\n",
        "    \"\"\"Calculate performance metrics for each detector/descriptor combination\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for combo in df.groupby(['Detector', 'Descriptor']):\n",
        "        detector, descriptor = combo[0]\n",
        "        data = combo[1]\n",
        "\n",
        "        # Only calculate metrics where both TTC values are valid\n",
        "        valid_data = data.dropna(subset=['TTC_Lidar', 'TTC_Camera'])\n",
        "\n",
        "        if len(valid_data) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = np.mean(np.abs(valid_data['TTC_Lidar'] - valid_data['TTC_Camera']))\n",
        "        rmse = np.sqrt(np.mean((valid_data['TTC_Lidar'] - valid_data['TTC_Camera'])**2))\n",
        "\n",
        "        # Relative error (percentage)\n",
        "        rel_error = np.mean(np.abs(valid_data['TTC_Lidar'] - valid_data['TTC_Camera']) / valid_data['TTC_Lidar']) * 100\n",
        "\n",
        "        # Standard deviation of camera TTC (stability measure)\n",
        "        stability = np.std(valid_data['TTC_Camera'])\n",
        "\n",
        "        # Success rate (percentage of valid computations)\n",
        "        total_frames = len(data)\n",
        "        valid_frames = len(valid_data)\n",
        "        success_rate = (valid_frames / total_frames) * 100\n",
        "\n",
        "        # Outlier rate (>50% error)\n",
        "        outliers = np.sum(np.abs(valid_data['TTC_Lidar'] - valid_data['TTC_Camera']) / valid_data['TTC_Lidar'] > 0.5)\n",
        "        outlier_rate = (outliers / valid_frames) * 100 if valid_frames > 0 else 100\n",
        "\n",
        "        # Average keypoints and matches\n",
        "        avg_keypoints = np.mean(data['Keypoints'])\n",
        "        avg_matches = np.mean(data['Matches'])\n",
        "\n",
        "        results.append({\n",
        "            'Detector': detector,\n",
        "            'Descriptor': descriptor,\n",
        "            'Combination': f\"{detector}+{descriptor}\",\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'Relative_Error_%': rel_error,\n",
        "            'Stability_StdDev': stability,\n",
        "            'Success_Rate_%': success_rate,\n",
        "            'Outlier_Rate_%': outlier_rate,\n",
        "            'Avg_Keypoints': avg_keypoints,\n",
        "            'Avg_Matches': avg_matches,\n",
        "            'Valid_Frames': valid_frames,\n",
        "            'Total_Frames': total_frames\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def create_summary_table(metrics_df):\n",
        "    \"\"\"Create a ranked summary table\"\"\"\n",
        "    # Create ranking score (lower is better)\n",
        "    # Weighted combination of MAE, stability, and outlier rate\n",
        "    metrics_df['Ranking_Score'] = (\n",
        "        metrics_df['MAE'] * 0.4 +\n",
        "        metrics_df['Stability_StdDev'] * 0.3 +\n",
        "        metrics_df['Outlier_Rate_%'] * 0.01 +\n",
        "        (100 - metrics_df['Success_Rate_%']) * 0.02\n",
        "    )\n",
        "\n",
        "    # Sort by ranking score\n",
        "    summary = metrics_df.sort_values('Ranking_Score').copy()\n",
        "\n",
        "    # Add rank column\n",
        "    summary['Rank'] = range(1, len(summary) + 1)\n",
        "\n",
        "    # Select and reorder columns for summary\n",
        "    summary_cols = [\n",
        "        'Rank', 'Combination', 'MAE', 'RMSE', 'Relative_Error_%',\n",
        "        'Stability_StdDev', 'Success_Rate_%', 'Outlier_Rate_%',\n",
        "        'Avg_Keypoints', 'Avg_Matches'\n",
        "    ]\n",
        "\n",
        "    return summary[summary_cols].round(3)\n",
        "\n",
        "def create_comparison_table(df):\n",
        "    \"\"\"Create frame-by-frame comparison table\"\"\"\n",
        "    # Pivot to show TTC values for each combination across frames\n",
        "    lidar_pivot = df.pivot_table(\n",
        "        values='TTC_Lidar',\n",
        "        index='Frame',\n",
        "        columns=['Detector', 'Descriptor'],\n",
        "        aggfunc='first'\n",
        "    )\n",
        "\n",
        "    camera_pivot = df.pivot_table(\n",
        "        values='TTC_Camera',\n",
        "        index='Frame',\n",
        "        columns=['Detector', 'Descriptor'],\n",
        "        aggfunc='first'\n",
        "    )\n",
        "\n",
        "    # Create comparison showing both Lidar and Camera TTC\n",
        "    comparison_data = []\n",
        "\n",
        "    for frame in sorted(df['Frame'].unique()):\n",
        "        frame_data = {'Frame': frame}\n",
        "\n",
        "        # Add Lidar TTC (should be same for all combinations)\n",
        "        lidar_ttc = df[df['Frame'] == frame]['TTC_Lidar'].iloc[0] if len(df[df['Frame'] == frame]) > 0 else np.nan\n",
        "        frame_data['TTC_Lidar'] = lidar_ttc\n",
        "\n",
        "        # Add Camera TTC for each combination\n",
        "        for combo in df.groupby(['Detector', 'Descriptor']):\n",
        "            detector, descriptor = combo[0]\n",
        "            combo_data = combo[1]\n",
        "            combo_frame = combo_data[combo_data['Frame'] == frame]\n",
        "\n",
        "            if len(combo_frame) > 0:\n",
        "                camera_ttc = combo_frame['TTC_Camera'].iloc[0]\n",
        "                frame_data[f\"{detector}+{descriptor}\"] = camera_ttc\n",
        "            else:\n",
        "                frame_data[f\"{detector}+{descriptor}\"] = np.nan\n",
        "\n",
        "        comparison_data.append(frame_data)\n",
        "\n",
        "    return pd.DataFrame(comparison_data)\n",
        "\n",
        "def create_visualizations(df, metrics_df, output_dir=\"ttc_analysis_plots\"):\n",
        "    \"\"\"Create visualization plots\"\"\"\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "    plt.style.use('default')\n",
        "\n",
        "    # 1. Performance ranking bar chart\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    top_10 = metrics_df.sort_values('Ranking_Score').head(10)\n",
        "    bars = ax.barh(range(len(top_10)), top_10['MAE'])\n",
        "    ax.set_yticks(range(len(top_10)))\n",
        "    ax.set_yticklabels(top_10['Combination'])\n",
        "    ax.set_xlabel('Mean Absolute Error (seconds)')\n",
        "    ax.set_title('Top 10 Detector/Descriptor Combinations by MAE')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                f'{width:.3f}', ha='left', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/ranking_mae.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. TTC comparison scatter plot for best performer\n",
        "    best_combo = metrics_df.loc[metrics_df['MAE'].idxmin()]\n",
        "    best_data = df[(df['Detector'] == best_combo['Detector']) &\n",
        "                   (df['Descriptor'] == best_combo['Descriptor'])]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    scatter = ax.scatter(best_data['TTC_Lidar'], best_data['TTC_Camera'],\n",
        "                        alpha=0.6, s=50)\n",
        "\n",
        "    # Add diagonal line (perfect correlation)\n",
        "    min_val = min(best_data['TTC_Lidar'].min(), best_data['TTC_Camera'].min())\n",
        "    max_val = max(best_data['TTC_Lidar'].max(), best_data['TTC_Camera'].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect correlation')\n",
        "\n",
        "    ax.set_xlabel('TTC Lidar (seconds)')\n",
        "    ax.set_ylabel('TTC Camera (seconds)')\n",
        "    ax.set_title(f'TTC Correlation - Best Performer: {best_combo[\"Combination\"]}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/best_correlation.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Success rate vs MAE scatter\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    scatter = ax.scatter(metrics_df['Success_Rate_%'], metrics_df['MAE'],\n",
        "                        s=100, alpha=0.6, c=metrics_df['Avg_Keypoints'],\n",
        "                        cmap='viridis')\n",
        "\n",
        "    # Add labels for interesting points\n",
        "    for i, row in metrics_df.iterrows():\n",
        "        if row['MAE'] < 2 or row['Success_Rate_%'] > 95:  # Good performers\n",
        "            ax.annotate(row['Combination'], (row['Success_Rate_%'], row['MAE']),\n",
        "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    ax.set_xlabel('Success Rate (%)')\n",
        "    ax.set_ylabel('Mean Absolute Error (seconds)')\n",
        "    ax.set_title('Success Rate vs Accuracy Trade-off')\n",
        "    plt.colorbar(scatter, label='Average Keypoints')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/success_vs_accuracy.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Frame-by-frame comparison for top 5 methods\n",
        "    top_5 = metrics_df.sort_values('MAE').head(5)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    # Plot Lidar TTC as reference\n",
        "    frames = sorted(df['Frame'].unique())\n",
        "    lidar_ttc = [df[df['Frame'] == f]['TTC_Lidar'].iloc[0] if len(df[df['Frame'] == f]) > 0 else np.nan for f in frames]\n",
        "    ax.plot(frames, lidar_ttc, 'k-', linewidth=2, label='Lidar (Ground Truth)', marker='o')\n",
        "\n",
        "    # Plot top 5 camera methods\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, 5))\n",
        "    for i, (_, row) in enumerate(top_5.iterrows()):\n",
        "        combo_data = df[(df['Detector'] == row['Detector']) &\n",
        "                       (df['Descriptor'] == row['Descriptor'])]\n",
        "        ax.plot(combo_data['Frame'], combo_data['TTC_Camera'],\n",
        "                color=colors[i], label=f\"{row['Combination']} (MAE: {row['MAE']:.3f})\",\n",
        "                marker='s', alpha=0.7)\n",
        "\n",
        "    ax.set_xlabel('Frame Number')\n",
        "    ax.set_ylabel('Time to Collision (seconds)')\n",
        "    ax.set_title('TTC Estimates - Top 5 Performing Methods')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/frame_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Visualizations saved to {output_dir}/\")\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    csv_file = \"ttc_analysis_results.csv\"  # Your CSV file from C++ analysis\n",
        "\n",
        "    print(\"Loading and cleaning data...\")\n",
        "    df = load_and_clean_data(csv_file)\n",
        "    print(f\"Loaded {len(df)} valid data points\")\n",
        "\n",
        "    print(\"Calculating performance metrics...\")\n",
        "    metrics_df = calculate_performance_metrics(df)\n",
        "\n",
        "    print(\"Creating summary table...\")\n",
        "    summary_table = create_summary_table(metrics_df)\n",
        "\n",
        "    print(\"Creating comparison table...\")\n",
        "    comparison_table = create_comparison_table(df)\n",
        "\n",
        "    # Save tables to CSV\n",
        "    summary_table.to_csv(\"ttc_summary_table.csv\", index=False)\n",
        "    comparison_table.to_csv(\"ttc_comparison_table.csv\", index=False)\n",
        "    metrics_df.to_csv(\"ttc_detailed_metrics.csv\", index=False)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TTC ANALYSIS SUMMARY - TOP 10 PERFORMERS\")\n",
        "    print(\"=\"*80)\n",
        "    print(summary_table.head(10).to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"WORST 5 PERFORMERS\")\n",
        "    print(\"=\"*50)\n",
        "    print(summary_table.tail(5).to_string(index=False))\n",
        "\n",
        "    # Show some interesting statistics\n",
        "    print(f\"\\nBest performer: {summary_table.iloc[0]['Combination']} (MAE: {summary_table.iloc[0]['MAE']:.3f}s)\")\n",
        "    print(f\"Worst performer: {summary_table.iloc[-1]['Combination']} (MAE: {summary_table.iloc[-1]['MAE']:.3f}s)\")\n",
        "    print(f\"Most stable: {metrics_df.loc[metrics_df['Stability_StdDev'].idxmin(), 'Combination']} (StdDev: {metrics_df['Stability_StdDev'].min():.3f}s)\")\n",
        "    print(f\"Highest success rate: {metrics_df.loc[metrics_df['Success_Rate_%'].idxmax(), 'Combination']} ({metrics_df['Success_Rate_%'].max():.1f}%)\")\n",
        "\n",
        "    print(\"Creating visualizations...\")\n",
        "    create_visualizations(df, metrics_df)\n",
        "\n",
        "    print(f\"\\nAnalysis complete! Files saved:\")\n",
        "    print(f\"- ttc_summary_table.csv (ranked performance summary)\")\n",
        "    print(f\"- ttc_comparison_table.csv (frame-by-frame comparison)\")\n",
        "    print(f\"- ttc_detailed_metrics.csv (detailed metrics)\")\n",
        "    print(f\"- ttc_analysis_plots/ (visualization plots)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsudiLG-4G3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UUAatZ3V4PSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}